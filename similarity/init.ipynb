{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      ".gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 1.17MB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n",
      "README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 11.4MB/s]\n",
      "config.json: 100%|██████████| 571/571 [00:00<?, ?B/s] \n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 117kB/s]\n",
      "data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 19.8MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 438M/438M [00:58<00:00, 7.47MB/s] \n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 53.2kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 239kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.10MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 363/363 [00:00<?, ?B/s] \n",
      "train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 6.48MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 564kB/s]\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 346kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02250253 -0.0782918  -0.02303075 ... -0.00827929  0.02652693\n",
      "  -0.00201895]\n",
      " [ 0.04170237  0.00109741 -0.01553421 ... -0.02181627 -0.06359361\n",
      "  -0.00875286]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal = \"Machine learning is a branch of artificial intelligence focused on creating algorithms that enable computers to learn and improve from data. It involves training models to make predictions or decisions without explicit programming, relying on patterns and experiences gleaned from the provided information. The goal is continuous refinement of performance as more data becomes available.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = \"Machine learning empowers computers to learn from data, improving performance without explicit programming. It involves crafting algorithms that evolve through experience, enabling systems to make predictions or decisions. The essence lies in leveraging data for autonomous adaptation and continual enhancement.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.7826381325721741, Marks: 3.9\n"
     ]
    }
   ],
   "source": [
    "source_embedding = model.encode(ideal)\n",
    "comparison_sentences = [student]\n",
    "comparison_embeddings = model.encode(comparison_sentences)\n",
    "\n",
    "similarity_scores = util.pytorch_cos_sim(source_embedding, comparison_embeddings)[0].tolist()\n",
    "\n",
    "max_similarity = 1.0\n",
    "min_similarity = 0.0\n",
    "max_marks = 5\n",
    "\n",
    "\n",
    "for i, sentence in enumerate(comparison_sentences):\n",
    "    similarity_score = similarity_scores[i]\n",
    "    normalized_score = max(0, min(max_similarity, similarity_score))\n",
    "    marks = round(normalized_score * max_marks, 1)\n",
    "    print(f\"Similarity Score: {similarity_score}, Marks: {marks}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
